# 百度无人驾驶系统Apollo 2.0小解析

最近百度开放了Apollo 2.0的部分代码，小解析一下Apollo 2.0吧，边看文档边看代码同时记录整理，所以会比较慢，得分好几次才能写完～

最终理清楚整套系统运行环境、程序架构以及具体工作流程。

先说 **运行环境** ：硬件及操作系统。

![](https://pic1.zhimg.com/v2-6e14c1cbc231b8260c7ae0c25583c500_b.jpg)

## 硬件：

 **计算中心：** 就是整套系统大脑了，使用的是Neousys
Nuvo-6108GC，这是一款性能强大的X86解构工业控制计算机，强大到什么程度呢？支持至强E3和I7，支持GTX1080显卡，玩游戏的朋友都知道这款显卡，强大的性能我就不多说了，所以比较适合用来做自动驾驶。

 **CAN通讯卡** ，用来和汽车进行通讯以控制汽车的加速、制动、档位、方向等信号，使用的是ESD CAN-
PCIe/402-B4，CAN卡直接插在主机内。

 **GPS和IMU：** 定位系统和惯性制导，当然就是用来进行GPS定位和惯性定位，使用的是NovAtel SPAN-IGM-A1或者NovAtel
SPAN® ProPak6™ and NovAtel IMU-IGM-A1，通过串口连接主机。

 **激光雷达：** 扫描距离达到120米，水平360度扫描，垂直FOV26.9度，型号为Velodyne HDL-64E S3，通过以太网连接计算中心。

 **摄像头：** 用于视觉，Leopard Imaging LI-USB30-AR023ZWDR with USB 3.0 case，通过USB连接主机。

 **毫米波雷达：** 用于探测前方，安装在车辆前端，为大陆集团的ARS408-21，连接至CAN卡。

除去以上这些，还需要鼠标键盘显示器，当然就是平常用的那种。

## 软件：

 **操作系统：** Ubuntu Linux 推荐版本14.04，看到这个操作系统，我非常兴奋，因为我曾经在这个操作系统下Coding4年。

然后安装Apollo 内核和CAN卡、显卡驱动。

下图是Apollo运行图。

![](https://pic2.zhimg.com/v2-0e6ab3d3a41d4ad940102a31e52fb835_b.jpg)

 **整个软件包含如下模块**

![](https://pic4.zhimg.com/v2-3b5552d76bd225ec19ccac767f693be9_b.jpg)

calibration：校准模块，使用前必须对系统进行校准和标定，包括激光雷达与摄像头、毫米波雷达与摄像头等。所谓校准就是要对齐激光雷达、摄像头以及毫米波雷达获得的信息，我们知道激光雷达可以获得详细的3D信息，但是不能获得颜色信息，摄像头可以获得颜色信息，但是无法获得深度等3D信息，毫米波雷达不能获得颜色信息，但是可以获得3D信息，三者获得的信息对齐后，就可以同时获得实际环境中的3D信息和颜色信息。下图就是对齐后的激光雷达和摄像头画面，下下图为对齐后的毫米波雷达和激光雷达画面。

![](https://pic4.zhimg.com/v2-5fb5baea0e1552891a99c70934c433e0_b.jpg)

![](https://pic3.zhimg.com/v2-0057b86a6490cc0270d82cd3252bdf99_b.jpg)

canbus：管理CAN卡和CAN通讯，把接受到的信号传递给相应模块，同时将Control模块的命令下发到车辆。

common：其他模块之外的代码都在这里

control：主控制模块，基于车道规划和车辆当前状态，输出转向、加速和制动控制信号到CAN卡

data Data: 收集、存储、处理收集到的各种数据。

dreamview：这是一个Web应用，可以帮助用户随时掌握系统的输出数据，包括车道、位置、车身等情况。

drivers：此模块包含CAN卡、激光雷达、毫米波雷达、GPS以及摄像头等相关设备的驱动程序

e2e：end to
end，端到端深度学习，所谓e2e指的是由传感器的输入，直接决定车的行为，例如油门，刹车，方向等。也就是机器学习的算法直接学习人类司机的驾驶行为。这部分在代码中需要另外下载，学习的数据主要来源于传感器的原始数据，包括图像、激光雷达、雷达等。end-
to-end输入以图像为主。 输出是车辆的控制决策指令，如方向盘角度、加速、刹车。
连接输入输出的是深度神经网络，即通过神经网络直接生成车辆控制指令对车辆进行横向控制和纵向控制，中间没有人工参与的逻辑程序。横向控制，主要是指通过方向盘控制车身横向移动，即方向盘角度。纵向控制，是指通过油门和刹车控制车身纵向的移动，即加速、刹车等。横向模型的输出没有采用方向盘角度，而是使用要行驶的曲率（即拐弯半径的倒数）。

elo：百度的车辆自身定位部分，结构如下。

![](https://pic4.zhimg.com/v2-3ea8301a48befa27c8735dd621bbfc0c_b.jpg)

这部分的代码也是另外下载。前向的摄像头会采集车道数据以实现更精确的定位，输出的位置信息包括车辆的x y z坐标，还有就是在百度高精度地图中的ID。

localization：车辆定位服务，包含两种定位方式，一种是GPS和IMU，另一种是多传感器融合。输出位置估算结构体。

map：地图

monitor：此模块用于监测硬件状态及整个系统的健康程度。

perception:此模块用于障碍物感知和红绿灯等交通信号的感知，主要依靠摄像头数据以及激光雷达和毫米波雷达的数据，当然还有高精度地图，输出3D的障碍物信息，包括方向、速度和障碍物类型，当然还有红绿灯等交通信号。模块可以将障碍物标注为机动车、非机动车、行人和其他，使用的是激光点云算法。

planning：根据车辆位置和车辆状态、地图、障碍物、导航信息等计算具体的车道。规划车道有两种方式，一种是提前把轨迹存入程序，然后根据车辆状态和位置提取轨迹另外一种是实时计算。

prediction：根据障碍物的位置、航向、速度、加速度计算障碍物的可能轨迹。

routing：路径规划，根据地图和起点终点位置计算出具体的导航信息。

third_party_perception：第三方感知

tools：工具

 **根据这些模块，可以大概看出整体流程了：**

首先，用户输入目的地， **routing** 模块就可以根据终点位置计算出具体的导航信息。激光雷达、毫米波雷达和摄像头拍摄到的数据配合高精度地图由
**percepting** 模块计算出3D的障碍物信息并识别交通标志及交通信号，这些数据进入 **perdiction**
模块，计算出障碍物的可能轨迹，如此就可以结合以上信息并根据车辆定位模块 **localizationg**
提供的车辆位置由planning模块得到车辆应该走的具体车道。

得到车道后车辆 **control** 模块结合车辆的当前状态计算加速、刹车和方向的操作信号，此信号进入CAN卡后输出到车内，如此实现了车辆的自动驾驶。

在整个流程中， **monitor** 模块会及时监测硬件及系统的健康状况，出现问题肯定就会中止驾驶过程。对于驾驶中的信息，用户可以通过web应用
**dreamview** 来查看，下图就是实际驾驶过程中的dreamview界面。

![](https://pic3.zhimg.com/v2-93344515b6491fb53cd2747460dc69a9_b.jpg)

说说写到这里的一些小想法吧。

这套软件，很明显只是一个框架，而且开放的也是整套系统中的部分代码，并没有多少核心重要的东西在里面，也是非常初级的智能驾驶，顶多就是用来测试，我倾向于认为百度不过是渲染下气氛，更多的是传播上的考虑，如果你指望这些代码搞定一辆实用的无人驾驶车，几乎是不可能的。

对于百度来说，他很可能是做出一个姿态来吸引更多厂商的合作和尝试，当然不可谓不好，但是也并没多像大家想的那么美好。

对于厂商来说，如果使用了百度这一套，百度基本上就可以和这个入口捆绑在一起了，主机厂也和百度绑在一起了，无人驾驶可是未来出行的核心，各大互联网巨头肯定都要全力抢夺，百度走的够快啊！

以上还有一些需要细化补充的地方，有空继续写...

更多详情: <https://zhuanlan.zhihu.com/p/32511462>



---
### TAGS
{AI*AG概述}

---
### NOTE ATTRIBUTES
>Created Date: 2018-01-08 16:13:20  
>Last Evernote Update Date: 2019-04-06 02:07:18  
>source: desktop.win  
>source-application: evernote.win32  